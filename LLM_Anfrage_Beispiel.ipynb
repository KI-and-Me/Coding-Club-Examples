{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachmodell-Anfrage mit modernem OpenAI Client\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert, wie man mit Hilfe des OpenAI-Clients eine Verbindung zu einem Sprachmodell-Server aufbaut und Anfragen sendet. Es verwendet den Hochschulinternen API-Endpunkt.\n",
    "\n",
    "## Überblick\n",
    "\n",
    "In diesem Notebook lernen Sie:\n",
    "1. Wie man die OpenAI-Bibliothek installiert\n",
    "2. Wie man den modernen OpenAI Client mit benutzerdefinierten Endpunkten konfiguriert\n",
    "3. Wie man verfügbare Sprachmodelle abfragt\n",
    "4. Wie man verschiedene Arten von Anfragen an ein ausgewähltes Modell sendet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation der OpenAI-Bibliothek\n",
    "\n",
    "Zunächst müssen wir sicherstellen, dass die OpenAI-Bibliothek installiert ist. \n",
    "\n",
    "Auf AI.H2.de ist das nicht nötig, da der JupyterHub die Bibliothek schon vorinstalliert.\n",
    "\n",
    "In anderen Umgebungen muss das noch nachgeholt werden. \n",
    "Falls Sie sie noch nicht installiert haben, entfernen Sie die `#` am Anfang der Zeile `!pip install openai` und führen Sie die folgende Zelle aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der OpenAI-Bibliothek (mindestens Version 1.0.0)\n",
    "#!pip install \"openai>=1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialisierung des modernen OpenAI Clients\n",
    "Wir importieren die `OpenAI`-Klasse und erstellen einen Client mit unseren Konfigurationsparametern.\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "- `api_key`: Der Authentifizierungsschlüssel für die API\n",
    "- `base_url`: Die Basis-URL des API-Servers (ersetzt das frühere `api_base`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der OpenAI Client-Klasse\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialisierung des Clients mit benutzerdefinierten Parametern\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-1234\",            # API-Key für die Authentifizierung\n",
    "    base_url=\"https://ai.h2.de/llm\"  # Benutzerdefinierter Endpunkt (ersetzt api_base)\n",
    ")\n",
    "\n",
    "print(f\"Moderner OpenAI Client initialisiert mit Basis-URL: {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verfügbare Modelle abfragen\n",
    "\n",
    "Als nächstes fragen wir ab, welche Sprachmodelle auf dem Server verfügbar sind. Mit dem modernen Client verwenden wir die `models.list()`-Methode des Client-Objekts anstelle des globalen `openai.Model.list()`.\n",
    "\n",
    "**Technische Details:**\n",
    "- Der Aufruf `client.models.list()` sendet eine GET-Anfrage an den Endpunkt `/models`\n",
    "- Die Antwort enthält Metadaten zu jedem verfügbaren Modell\n",
    "- Wir extrahieren die Modell-IDs, um später eine Auswahl zu treffen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Verfügbare Modelle mit dem modernen Client auflisten\n",
    "    models = client.models.list()\n",
    "    \n",
    "    print(\"Verfügbare Modelle:\")\n",
    "    for model in models.data:\n",
    "        print(f\"- {model.id}\")\n",
    "    \n",
    "    # Automatische Auswahl des ersten verfügbaren Modells\n",
    "    if models.data:\n",
    "        selected_model = models.data[0].id\n",
    "        print(f\"\\nAutomatisch ausgewähltes Modell: {selected_model}\")\n",
    "    else:\n",
    "        print(\"\\nKeine Modelle verfügbar\")\n",
    "        selected_model = None\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Modelle: {e}\")\n",
    "    # Fallback auf ein Standardmodell, falls die Modellliste nicht abgerufen werden kann\n",
    "    selected_model = \"gpt-3.5-turbo\"  # Standardmodell (könnte auf dem Server verfügbar sein)\n",
    "    print(f\"Verwende Fallback-Modell: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eine einfache Anfrage senden\n",
    "\n",
    "Nun senden wir eine einfache Anfrage an das ausgewählte Modell. Mit dem modernen Client verwenden wir `client.chat.completions.create()` anstelle des globalen `openai.ChatCompletion.create()`.\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "- `model`: Die ID des zu verwendenden Modells\n",
    "- `messages`: Eine Liste von Nachrichten, die die Konversation darstellen\n",
    "- `temperature`: Steuert die Kreativität/Zufälligkeit der Antworten (0.0 bis 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "if selected_model:\n",
    "    try:\n",
    "        # Eine einfache Anfrage mit dem modernen Client\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Erkläre künstliche Intelligenz in 3 Sätzen.\"}\n",
    "            ],\n",
    "            temperature=0.7,  # Mittelwert für ausgewogene Kreativität und Präzision\n",
    "            max_tokens=150    # Begrenzt die Länge der Antwort\n",
    "        )\n",
    "        \n",
    "        # Antwort und Metadaten mit Markdown formatieren\n",
    "        response_content = completion.choices[0].message.content\n",
    "        \n",
    "        markdown_output = f\"\"\"\n",
    "## Antwort vom Modell:\n",
    "\n",
    "> {response_content}\n",
    "\n",
    "### Metadaten zur Antwort:\n",
    "- **Modell**: {completion.model}\n",
    "- **Completion-Tokens**: {completion.usage.completion_tokens}\n",
    "- **Prompt-Tokens**: {completion.usage.prompt_tokens}\n",
    "- **Gesamtanzahl der Tokens**: {completion.usage.total_tokens}\n",
    "\"\"\"\n",
    "        \n",
    "        # Formatierte Ausgabe anzeigen\n",
    "        display(Markdown(markdown_output))\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Fehler bei der Anfrage:** {e}\"))\n",
    "else:\n",
    "    display(Markdown(\"**Hinweis:** Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Eine komplexere Anfrage mit Systemanweisungen\n",
    "\n",
    "Wir können die Qualität und Art der Antworten verbessern, indem wir Systemanweisungen hinzufügen. Diese teilen dem Modell mit, wie es antworten soll, bevor der Benutzer eine Frage stellt.\n",
    "\n",
    "**Nachrichtentypen:**\n",
    "- `system`: Anweisungen, die das Verhalten des Modells definieren\n",
    "- `user`: Die eigentliche Anfrage des Benutzers\n",
    "- `assistant`: Frühere Antworten des Modells (für mehrstufige Konversationen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "if selected_model:\n",
    "    try:\n",
    "        # Eine komplexere Anfrage mit Systemanweisungen\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=[\n",
    "                # Systemanweisung für die Persona und den Stil\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent, der Programmierkonzepte einfach und präzise erklärt. Verwende wenn möglich Python-Beispielcode.\"},\n",
    "                # Die eigentliche Anfrage des Benutzers\n",
    "                {\"role\": \"user\", \"content\": \"Was ist der Unterschied zwischen einer Liste und einem Dictionary in Python?\"}\n",
    "            ],\n",
    "            temperature=0.5,  # Etwas niedrigere Temperatur für präzisere technische Erklärungen\n",
    "            max_tokens=300    # Mehr Tokens für ausführlichere Antworten\n",
    "        )\n",
    "        \n",
    "        # Antwort und Details mit Markdown formatieren\n",
    "        response_content = completion.choices[0].message.content\n",
    "        \n",
    "        markdown_output = f\"\"\"\n",
    "## Antwort vom Modell (mit Systemanweisung):\n",
    "\n",
    "> {response_content}\n",
    "\n",
    "### Anfrage-Details:\n",
    "- **Systemanweisung**: Erklärung von Programmierkonzepten mit Python-Beispielcode\n",
    "- **Benutzerfrage**: Unterschied zwischen Liste und Dictionary in Python\n",
    "- **Temperatur**: 0.5 (für präzisere technische Erklärungen)\n",
    "- **Max. Tokens**: 300 (für ausführlichere Antworten)\n",
    "\"\"\"\n",
    "        \n",
    "        # Formatierte Ausgabe anzeigen\n",
    "        display(Markdown(markdown_output))\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Fehler bei der Anfrage:** {e}\"))\n",
    "else:\n",
    "    display(Markdown(\"**Hinweis:** Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mehrstufige Konversation simulieren\n",
    "\n",
    "Wir können auch mehrstufige Konversationen simulieren, indem wir frühere Nachrichten in unsere Anfrage einbeziehen. Dies ist besonders nützlich, um Kontext beizubehalten und natürlichere Gespräche zu führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "if selected_model:\n",
    "    try:\n",
    "        # Eine mehrstufige Konversation\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"Du bist ein freundlicher und geduldiger Python-Tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Wie erstelle ich eine Liste in Python?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"In Python kannst du eine Liste auf verschiedene Weisen erstellen:\\n\\n1. Leere Liste: `my_list = []`\\n2. Liste mit Elementen: `my_list = [1, 2, 3, 'Hallo', True]`\\n3. Liste mit der list()-Funktion: `my_list = list((1, 2, 3))`\\n\\nListen sind veränderbar und können verschiedene Datentypen enthalten. Du kannst mit ihnen arbeiten, indem du auf Elemente zugreifst, sie hinzufügst oder entfernst.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Und wie füge ich neue Elemente hinzu?\"}\n",
    "        ]\n",
    "        \n",
    "        # Anfrage mit Konversationsverlauf\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=conversation,  # Der gesamte Konversationsverlauf\n",
    "            temperature=0.6\n",
    "        )\n",
    "        \n",
    "        # Antwort und Konversationsverlauf mit Markdown formatieren\n",
    "        response_content = completion.choices[0].message.content\n",
    "        \n",
    "        # Konversationsverlauf formatieren\n",
    "        conversation_markdown = \"\"\n",
    "        for message in conversation:\n",
    "            role = message[\"role\"].capitalize()\n",
    "            content = message[\"content\"]\n",
    "            conversation_markdown += f\"**{role}:** {content}\\n\\n\"\n",
    "        \n",
    "        markdown_output = f\"\"\"\n",
    "## Mehrstufige Konversation\n",
    "\n",
    "### Verlauf der Konversation:\n",
    "{conversation_markdown}\n",
    "\n",
    "### Aktuelle Antwort:\n",
    "> {response_content}\n",
    "\n",
    "### Konversationsdetails:\n",
    "- **Rollen**: System (Tutor), Benutzer, Assistent\n",
    "- **Temperatur**: 0.6\n",
    "- **Modell**: {selected_model}\n",
    "\"\"\"\n",
    "        \n",
    "        # Formatierte Ausgabe anzeigen\n",
    "        display(Markdown(markdown_output))\n",
    "        \n",
    "        # Wir könnten die Antwort des Modells zur Konversation hinzufügen, um sie fortzusetzen\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Fehler bei der Anfrage:** {e}\"))\n",
    "else:\n",
    "    display(Markdown(\"**Hinweis:** Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sehr komplexer Systemprompt mit Chain-of-Thought\n",
    "In diesem recht komplexen Beispiel, wird das Sprachmodell durch einen ausführlichen Systemprompt dazu gebracht, zunächst über das Ergebnis \"Nachzudenken\". Die Prompting-Technik nennt sich Chain-of-Though und ist die Basis für das, was \"Reasoning\"-Modelle automatisch tun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def demonstrate_chain_of_thought(client, model_id, problem, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Demonstriert Chain-of-Thought-Prompting, bei dem das Modell aufgefordert wird,\n",
    "    seinen Denkprozess Schritt für Schritt zu erklären.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI Client-Instanz\n",
    "        model_id: ID des zu verwendenden Modells\n",
    "        problem: Das zu lösende Problem oder die Frage\n",
    "        temperature: Temperatur für die Anfrage (0.0 bis 1.0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Chain-of-Thought Prompt erstellen\n",
    "        cot_prompt = f\"\"\"Denke Schritt für Schritt über folgendes Problem nach:\n",
    "        \n",
    "{problem}\n",
    "\n",
    "Gehe bei deinem Denkprozess wie folgt vor:\n",
    "1. Verstehe das Problem vollständig\n",
    "2. Identifiziere die wichtigen Informationen und Zusammenhänge\n",
    "3. Überlege verschiedene Ansätze zur Lösung\n",
    "4. Wähle den besten Ansatz und führe ihn aus\n",
    "5. Überprüfe dein Ergebnis\n",
    "6. Fasse deine Antwort zusammen\n",
    "\n",
    "Zeige alle Schritte deines Denkprozesses.\"\"\"\n",
    "        \n",
    "        # Anfrage mit CoT-Prompt\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein analytischer Assistent, der Probleme durch sorgfältiges, schrittweises Denken löst.\"},\n",
    "                {\"role\": \"user\", \"content\": cot_prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=800  # Mehr Tokens für ausführliche Antworten\n",
    "        )\n",
    "        \n",
    "        # Antwort mit Markdown formatieren\n",
    "        response_content = completion.choices[0].message.content\n",
    "        \n",
    "        # Identifiziere die verschiedenen Denkschritte für bessere Formatierung\n",
    "        thinking_steps = response_content.split(\"\\n\\n\")\n",
    "        formatted_steps = \"\"\n",
    "        \n",
    "        for i, step in enumerate(thinking_steps):\n",
    "            if step.strip():  # Ignoriere leere Zeilen\n",
    "                formatted_steps += f\"### Schritt {i+1}\\n{step}\\n\\n\"\n",
    "        \n",
    "        markdown_output = f\"\"\"\n",
    "## Chain-of-Thought Demonstration\n",
    "\n",
    "### Problem:\n",
    "> {problem}\n",
    "\n",
    "### Denkprozess des Modells:\n",
    "{formatted_steps}\n",
    "\n",
    "### Metadaten:\n",
    "- **Modell**: {model_id}\n",
    "- **Temperatur**: {temperature}\n",
    "- **Prompt-Technik**: Chain-of-Thought (CoT)\n",
    "- **Tokens**: {completion.usage.total_tokens} (gesamt)\n",
    "\"\"\"\n",
    "        \n",
    "        # Formatierte Ausgabe anzeigen\n",
    "        display(Markdown(markdown_output))\n",
    "        \n",
    "        return completion\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"**Fehler bei der Chain-of-Thought Anfrage:** {e}\"))\n",
    "        return None\n",
    "\n",
    "# Beispiel für die Verwendung\n",
    "if selected_model:\n",
    "    # Komplexes Problem, das schrittweises Denken erfordert\n",
    "    problem = \"\"\"\n",
    "    Ein Landwirt hat drei Säcke mit Getreide. Der erste Sack enthält 20 kg Weizen, \n",
    "    der zweite Sack enthält 35 kg Roggen und der dritte Sack enthält 15 kg Hafer.\n",
    "    \n",
    "    Der Landwirt verkauft Weizen für 2,50 € pro kg, Roggen für 2,00 € pro kg und \n",
    "    Hafer für 1,80 € pro kg.\n",
    "    \n",
    "    Wenn der Landwirt genau die Hälfte jedes Getreidesacks verkauft und den Rest \n",
    "    behält, wie viel Geld nimmt er ein und wie viel kg Getreide behält er insgesamt?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chain-of-Thought Demonstration ausführen\n",
    "    cot_result = demonstrate_chain_of_thought(\n",
    "        client=client,\n",
    "        model_id=selected_model,\n",
    "        problem=problem,\n",
    "        temperature=0.5  # Niedrigere Temperatur für logischere Antworten\n",
    "    )\n",
    "else:\n",
    "    display(Markdown(\"**Hinweis:** Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiterführende Ressourcen\n",
    "\n",
    "- [Offizielle OpenAI API Dokumentation](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Python Bibliothek auf GitHub](https://github.com/openai/openai-python)\n",
    "- [OpenAI Python API auf PyPI](https://pypi.org/project/openai/)\n",
    "- [OpenAI Migration Guide zur neuen Client-API](https://github.com/openai/openai-python/discussions/418)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

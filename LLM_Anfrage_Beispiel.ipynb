{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachmodell-Anfrage mit modernem OpenAI Client\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert, wie man mit Hilfe des OpenAI-Clients eine Verbindung zu einem Sprachmodell-Server aufbaut und Anfragen sendet. Es verwendet den Hochschulinternen API-Endpunkt.\n",
    "\n",
    "## Überblick\n",
    "\n",
    "In diesem Notebook lernen Sie:\n",
    "1. Wie man die OpenAI-Bibliothek installiert\n",
    "2. Wie man den modernen OpenAI Client mit benutzerdefinierten Endpunkten konfiguriert\n",
    "3. Wie man verfügbare Sprachmodelle abfragt\n",
    "4. Wie man verschiedene Arten von Anfragen an ein ausgewähltes Modell sendet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation der OpenAI-Bibliothek\n",
    "\n",
    "Zunächst müssen wir sicherstellen, dass die OpenAI-Bibliothek installiert ist. \n",
    "\n",
    "Auf AI.H2.de ist das nicht nötig, da der JupyterHub die Bibliothek schon vorinstalliert.\n",
    "\n",
    "In anderen Umgebungen muss das noch nachgeholt werden. \n",
    "Falls Sie sie noch nicht installiert haben, entfernen Sie die `#` am Anfang der Zeile `!pip install openai` und führen Sie die folgende Zelle aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der OpenAI-Bibliothek (mindestens Version 1.0.0)\n",
    "!pip install \"openai>=1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialisierung des modernen OpenAI Clients\n",
    "Wir importieren die `OpenAI`-Klasse und erstellen einen Client mit unseren Konfigurationsparametern.\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "- `api_key`: Der Authentifizierungsschlüssel für die API\n",
    "- `base_url`: Die Basis-URL des API-Servers (ersetzt das frühere `api_base`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der OpenAI Client-Klasse\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Initialisierung des Clients mit benutzerdefinierten Parametern\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-1234\",            # API-Key für die Authentifizierung\n",
    "    base_url=\"https://ai.h2.de/llm\"  # Benutzerdefinierter Endpunkt (ersetzt api_base)\n",
    ")\n",
    "\n",
    "print(f\"Moderner OpenAI Client initialisiert mit Basis-URL: {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verfügbare Modelle abfragen\n",
    "\n",
    "Als nächstes fragen wir ab, welche Sprachmodelle auf dem Server verfügbar sind. Mit dem modernen Client verwenden wir die `models.list()`-Methode des Client-Objekts anstelle des globalen `openai.Model.list()`.\n",
    "\n",
    "**Technische Details:**\n",
    "- Der Aufruf `client.models.list()` sendet eine GET-Anfrage an den Endpunkt `/models`\n",
    "- Die Antwort enthält Metadaten zu jedem verfügbaren Modell\n",
    "- Wir extrahieren die Modell-IDs, um später eine Auswahl zu treffen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Verfügbare Modelle mit dem modernen Client auflisten\n",
    "    models = client.models.list()\n",
    "    \n",
    "    print(\"Verfügbare Modelle:\")\n",
    "    for model in models.data:\n",
    "        print(f\"- {model.id}\")\n",
    "    \n",
    "    # Automatische Auswahl des ersten verfügbaren Modells\n",
    "    if models.data:\n",
    "        selected_model = models.data[0].id\n",
    "        print(f\"\\nAutomatisch ausgewähltes Modell: {selected_model}\")\n",
    "    else:\n",
    "        print(\"\\nKeine Modelle verfügbar\")\n",
    "        selected_model = None\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Abrufen der Modelle: {e}\")\n",
    "    # Fallback auf ein Standardmodell, falls die Modellliste nicht abgerufen werden kann\n",
    "    selected_model = \"gpt-3.5-turbo\"  # Standardmodell (könnte auf dem Server verfügbar sein)\n",
    "    print(f\"Verwende Fallback-Modell: {selected_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eine einfache Anfrage senden\n",
    "\n",
    "Nun senden wir eine einfache Anfrage an das ausgewählte Modell. Mit dem modernen Client verwenden wir `client.chat.completions.create()` anstelle des globalen `openai.ChatCompletion.create()`.\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "- `model`: Die ID des zu verwendenden Modells\n",
    "- `messages`: Eine Liste von Nachrichten, die die Konversation darstellen\n",
    "- `temperature`: Steuert die Kreativität/Zufälligkeit der Antworten (0.0 bis 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model:\n",
    "    try:\n",
    "        # Eine einfache Anfrage mit dem modernen Client\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Erkläre künstliche Intelligenz in 3 Sätzen.\"}\n",
    "            ],\n",
    "            temperature=0.7,  # Mittelwert für ausgewogene Kreativität und Präzision\n",
    "            max_tokens=150    # Begrenzt die Länge der Antwort\n",
    "        )\n",
    "        \n",
    "        # Antwort ausgeben\n",
    "        print(\"Antwort vom Modell:\")\n",
    "        print(\"-------------------\")\n",
    "        print(completion.choices[0].message.content)\n",
    "        print(\"-------------------\\n\")\n",
    "        \n",
    "        # Details zur Antwort anzeigen\n",
    "        print(\"Metadaten zur Antwort:\")\n",
    "        print(f\"Modell: {completion.model}\")\n",
    "        print(f\"Completion-Tokens: {completion.usage.completion_tokens}\")\n",
    "        print(f\"Prompt-Tokens: {completion.usage.prompt_tokens}\")\n",
    "        print(f\"Gesamtanzahl der Tokens: {completion.usage.total_tokens}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Anfrage: {e}\")\n",
    "else:\n",
    "    print(\"Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Eine komplexere Anfrage mit Systemanweisungen\n",
    "\n",
    "Wir können die Qualität und Art der Antworten verbessern, indem wir Systemanweisungen hinzufügen. Diese teilen dem Modell mit, wie es antworten soll, bevor der Benutzer eine Frage stellt.\n",
    "\n",
    "**Nachrichtentypen:**\n",
    "- `system`: Anweisungen, die das Verhalten des Modells definieren\n",
    "- `user`: Die eigentliche Anfrage des Benutzers\n",
    "- `assistant`: Frühere Antworten des Modells (für mehrstufige Konversationen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model:\n",
    "    try:\n",
    "        # Eine komplexere Anfrage mit Systemanweisungen\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=[\n",
    "                # Systemanweisung für die Persona und den Stil\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent, der Programmierkonzepte einfach und präzise erklärt. Verwende wenn möglich Python-Beispielcode.\"},\n",
    "                # Die eigentliche Anfrage des Benutzers\n",
    "                {\"role\": \"user\", \"content\": \"Was ist der Unterschied zwischen einer Liste und einem Dictionary in Python?\"}\n",
    "            ],\n",
    "            temperature=0.5,  # Etwas niedrigere Temperatur für präzisere technische Erklärungen\n",
    "            max_tokens=300    # Mehr Tokens für ausführlichere Antworten\n",
    "        )\n",
    "        \n",
    "        # Antwort ausgeben\n",
    "        print(\"Antwort vom Modell (mit Systemanweisung):\")\n",
    "        print(\"-------------------\")\n",
    "        print(completion.choices[0].message.content)\n",
    "        print(\"-------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Anfrage: {e}\")\n",
    "else:\n",
    "    print(\"Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mehrstufige Konversation simulieren\n",
    "\n",
    "Wir können auch mehrstufige Konversationen simulieren, indem wir frühere Nachrichten in unsere Anfrage einbeziehen. Dies ist besonders nützlich, um Kontext beizubehalten und natürlichere Gespräche zu führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model:\n",
    "    try:\n",
    "        # Eine mehrstufige Konversation\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"Du bist ein freundlicher und geduldiger Python-Tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Wie erstelle ich eine Liste in Python?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"In Python kannst du eine Liste auf verschiedene Weisen erstellen:\\n\\n1. Leere Liste: `my_list = []`\\n2. Liste mit Elementen: `my_list = [1, 2, 3, 'Hallo', True]`\\n3. Liste mit der list()-Funktion: `my_list = list((1, 2, 3))`\\n\\nListen sind veränderbar und können verschiedene Datentypen enthalten. Du kannst mit ihnen arbeiten, indem du auf Elemente zugreifst, sie hinzufügst oder entfernst.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Und wie füge ich neue Elemente hinzu?\"}\n",
    "        ]\n",
    "        \n",
    "        # Anfrage mit Konversationsverlauf\n",
    "        completion = client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=conversation,  # Der gesamte Konversationsverlauf\n",
    "            temperature=0.6\n",
    "        )\n",
    "        \n",
    "        # Antwort ausgeben\n",
    "        print(\"Antwort in einer mehrstufigen Konversation:\")\n",
    "        print(\"-------------------\")\n",
    "        print(completion.choices[0].message.content)\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "        # Wir könnten die Antwort des Modells zur Konversation hinzufügen, um sie fortzusetzen\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Anfrage: {e}\")\n",
    "else:\n",
    "    print(\"Kein Modell ausgewählt. Bitte stellen Sie sicher, dass Modelle verfügbar sind.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weiterführende Ressourcen\n",
    "\n",
    "- [Offizielle OpenAI API Dokumentation](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Python Bibliothek auf GitHub](https://github.com/openai/openai-python)\n",
    "- [OpenAI Python API auf PyPI](https://pypi.org/project/openai/)\n",
    "- [OpenAI Migration Guide zur neuen Client-API](https://github.com/openai/openai-python/discussions/418)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

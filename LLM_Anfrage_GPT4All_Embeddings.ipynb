{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c64436",
   "metadata": {},
   "source": [
    "\n",
    "# Sprachmodell-Anfrage mit modernem OpenAI Client und GPT4All-Embeddings\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert, wie man mit Hilfe des OpenAI-Clients und GPT4All-Embeddings\n",
    "eine umfassende Textanalyse durchführt. Es verwendet den Hochschulinternen API-Endpunkt.\n",
    "\n",
    "## Überblick\n",
    "\n",
    "In diesem Notebook lernen Sie:\n",
    "1. Wie man die OpenAI-Bibliothek und GPT4All installiert\n",
    "2. Wie man den modernen OpenAI Client mit benutzerdefinierten Endpunkten konfiguriert\n",
    "3. Wie man lokale Embeddings mit GPT4All generiert\n",
    "4. Wie man lange Texte analysiert und mit OpenAI integriert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b76953",
   "metadata": {},
   "source": [
    "## 1. Installation der erforderlichen Pakete\n",
    "\n",
    "Zunächst müssen wir sicherstellen, dass alle notwendigen Bibliotheken installiert sind.\n",
    "\n",
    "Auf AI.H2.de ist das nicht nötig, da der JupyterHub die Bibliotheken schon vorinstalliert.\n",
    "\n",
    "In anderen Umgebungen muss das noch nachgeholt werden. Führen Sie die folgende Zelle aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97694c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"openai>=1.0.0\" gpt4all langchain langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db3daf",
   "metadata": {},
   "source": [
    "## 2. Initialisierung des modernen OpenAI Clients\n",
    "\n",
    "Wir importieren die `OpenAI`-Klasse und erstellen einen Client mit unseren Konfigurationsparametern.\n",
    "\n",
    "**Wichtige Parameter:**\n",
    "- `api_key`: Der Authentifizierungsschlüssel für die API\n",
    "- `base_url`: Die Basis-URL des API-Servers (ersetzt das frühere `api_base`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialisierung des Clients mit benutzerdefinierten Parametern\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-1234\",            # API-Key für die Authentifizierung\n",
    "    base_url=\"https://ai.h2.de/llm\"  # Benutzerdefinierter Endpunkt\n",
    ")\n",
    "\n",
    "print(f\"Moderner OpenAI Client initialisiert mit Basis-URL: {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b18cf5",
   "metadata": {},
   "source": [
    "## 3. Einrichtung des GPT4All-Embeddings-Modells\n",
    "\n",
    "Wir initialisieren das GPT4All-Embeddings-Modell. Dieses Modell ist besonders geeignet für:\n",
    "- Lokale Text-Einbettungen\n",
    "- Effiziente Verarbeitung langer Texte\n",
    "- Privatsphäre-gerechte Textanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import Embed4All\n",
    "\n",
    "# Initialisierung des Embeddings-Modells\n",
    "embedder = Embed4All()\n",
    "\n",
    "print(\"GPT4All-Embeddings-Modell erfolgreich initialisiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be0aad",
   "metadata": {},
   "source": [
    "## 4. Generierung von Embeddings für einen langen Text\n",
    "\n",
    "In diesem Abschnitt demonstrieren wir, wie man Embeddings für einen langen Text generiert. Das Modell verarbeitet automatisch Texte, die länger als der Kontext sind, indem es sie in Chunks unterteilt und die Embeddings durchschnittlich berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc58c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für einen langen Text (z.B. ein Artikel oder Dokument)\n",
    "long_text = \"\"\"\n",
    "Die Künstliche Intelligenz (KI) ist eine der wichtigsten Technologien des 21. Jahrhunderts.\n",
    "Sie revolutioniert Bereiche wie Gesundheitswesen, Finanzwesen, Produktion und Vertrieb.\n",
    "KI-Systeme können komplexe Muster erkennen, Vorhersagen treffen und selbstständig lernen.\n",
    "Dabei spielen verschiedene Techniken wie neuronale Netze, maschinelles Lernen und tiefes Lernen\n",
    "eine zentrale Rolle. Die Entwicklung fortschreitender KI-Modelle wie GPT-4 oder Claude\n",
    "öffnet neue Möglichkeiten für natürlichsprachliche Interaktionen und intelligente Systeme.\n",
    "Trotz der Vorteile gibt es auch Herausforderungen wie Datensicherheit, Ethik und\n",
    "die Notwendigkeit menschlicher Überwachung.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generieren der Embeddings\n",
    "embeddings = embedder.embed(long_text, long_text_mode=\"mean\")\n",
    "\n",
    "# Ausgabe der Embedding-Größe\n",
    "print(f\"Generierte Embeddings haben die Dimension: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653c9bf",
   "metadata": {},
   "source": [
    "## 5. Analyse der Embeddings\n",
    "\n",
    "Wir können die Embeddings nutzen, um Ähnlichkeiten zwischen Texten zu analysieren. Dafür verwenden wir den Cosinus-Ähnlichkeitsvergleich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Beispiel-Anfrage für Ähnlichkeitsvergleich\n",
    "query = \"Welche technischen Grundlagen stehen hinter modernen KI-Systemen?\"\n",
    "query_embedding = embedder.embed(query)\n",
    "\n",
    "# Cosinus-Ähnlichkeit berechnen\n",
    "similarity = cosine_similarity([query_embedding], [embeddings])[0][0]\n",
    "print(f\"Ähnlichkeit zwischen Anfrage und Dokument: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72c66b",
   "metadata": {},
   "source": [
    "## 6. Integration mit OpenAI für erweiterte Analyse\n",
    "\n",
    "Wir können die Erkenntnisse aus den Embeddings mit dem OpenAI-Modell kombinieren, um eine detailliertere Analyse durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae81f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Senden einer Anfrage an das OpenAI-Modell mit den Erkenntnissen aus den Embeddings\n",
    "analysis_request = f\"\"\"\n",
    "Basierend auf der Textanalyse mit GPT4All-Embeddings:\n",
    "- Der Text hat eine Embedding-Dimensionalität von {len(embeddings)}\n",
    "- Die Ähnlichkeit zur Anfrage '{query}' beträgt {similarity:.4f}\n",
    "\n",
    "Bitte gib eine detaillierte Analyse des Textinhalts und der Fragestellung.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Standardmodell für Analyse\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent, der bei der Analyse von Text-Embeddings hilft.\"},\n",
    "        {\"role\": \"user\", \"content\": analysis_request}\n",
    "    ],\n",
    "    temperature=0.3  # Geringe Temperatur für präzise Analyse\n",
    ")\n",
    "\n",
    "# Formatierte Ausgabe der Analyse\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "markdown_output = f\"\"\"\n",
    "## Analyse des Textes:\n",
    "\n",
    "> {completion.choices[0].message.content}\n",
    "\n",
    "### Metadaten zur Analyse:\n",
    "- **Modell**: {completion.model}\n",
    "- **Completion-Tokens**: {completion.usage.completion_tokens}\n",
    "- **Prompt-Tokens**: {completion.usage.prompt_tokens}\n",
    "- **Gesamtanzahl der Tokens**: {completion.usage.total_tokens}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
